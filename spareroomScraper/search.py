# Standard library imports
import sys
import time
import urllib.parse as urlparse

# Third party imports
from bs4 import BeautifulSoup
import requests

# Local imports
from spareroomScraper.custom_exceptions import BadSelector
from spareroomScraper.emailer import Emailer
from spareroomScraper.payload import Payload
from spareroomScraper.advert import Advert


DOMAIN = 'https://www.spareroom.co.uk/'
SEARCH_ENDPOINT = 'https://www.spareroom.co.uk/flatshare/'
ADVANCED_SEARCH_ENDPOINT = 'https://www.spareroom.co.uk/flatshare/search.pl'
ADVERTS_URLS_SELECTOR = '.listing-results-content.desktop > a'
ADVERTS_SELECTOR = '.panel-listing-result'

class Search:
    """ A class that scrapes Spareroom for adverts,
        by performing an advanced search against the
        ADVANCED_SEARCH_ENDPOINT.

        The payload is generated by instantiating the
        Payload class.
    """

    def __init__(self, config_file, number_of_pages, offset):
        """ Initialise the payload and get the search_id for
            the search criteria specified in the config file.

            Args:
                config_file: The path to the config file.
                number_of_pages: The number of pages to scrape.
                                 Each page contains 10 results.
        """

        self.config_file = config_file
        self.number_of_pages = number_of_pages
        self.offset = offset
        self.advanced_search_payload = self._get_advanced_search_payload()
        self.search_id = self._get_search_id()

    def _get_advanced_search_payload(self):
        """ Generate the payload for this search.

            Returns:
                The payload in a dictionary format.
        """

        payload = Payload(self.config_file)

        return payload.get_advanced_search_payload()

    def _get_search_id(self):
        """ Get the search id for this specific search.

            Returns:
                search_id: The id of the search.
        """

        response = self._make_request(ADVANCED_SEARCH_ENDPOINT, self.advanced_search_payload)

        url = response.url

        # Extract the search_id from the url
        url_parameters = urlparse.parse_qs(urlparse.urlparse(url).query)

        try:
            search_id = url_parameters['search_id'][0]
        except KeyError:
            print('Parameter name might have changed!')
            raise

        return search_id

    def _make_request(self, url, payload=None):
        """ Make a request to a specific endpoint.

            Args:
                url: The endpoint url
                payload: The payload in a dict format

            Returns:
                response: A requests.Response object.

            Raises:
                Raises stored HTTPError, if one occurred.
        """

        session = requests.session()

        response = session.get(url, params=payload)

        # Raise any HTTP status errors
        response.raise_for_status()

        # Be gentle with the requests!
        time.sleep(5)

        return response

    def _pull_pages(self):
        """ Pull a number of pages from the results of the search.

            Yields:
                response: A requests.Response object
        """

        previous_url = ''

        for i in range(0, self.number_of_pages*self.offset, self.offset):
            # Set up additional payload attributes.
            payload = {'offset': i,
                       'search_id': self.search_id,
                       'sort_by': 'days_since_placed',
                       'mode': 'list'
            }

            response = self._make_request(SEARCH_ENDPOINT, payload)

            # If response.url is the same as the url of the
            # previous request, then there are no more pages.
            # We can stop sending requests
            if previous_url == response.url:
                break
            else:
                previous_url = response.url

            yield response

    def _get_all_adverts_urls(self, response):
        """ Get the advert results of a single page.

            Args:
                response: A requests.Response object

            Yields:
                The url of each page returned by the search.

            Raises:
                BadSelector: If the CSS selector doesn't return any results.
        """

        page = BeautifulSoup(response.content, 'html.parser')

        adverts_urls = page.select(ADVERTS_URLS_SELECTOR)

        if not adverts_urls:
            raise BadSelector('The CSS selector for the urls might have changed!')

        for advert_url in adverts_urls: 
            # This is a bs4 object
            yield DOMAIN + advert_url['href']

    def _get_all_ads(self, response):
        """ Get the prices on a single page.

            Args:
                response: A requests.Response object

            Yields:
                All of the ads returned by the search.

            Raises:
                BadSelector: If the CSS selector doesn't return any results.
        """

        page = BeautifulSoup(response.content, 'html.parser')

        adverts = page.select(ADVERTS_SELECTOR)

        if not adverts:
            raise BadSelector('The CSS selector for the ads might have changed!')

        for ad_response in adverts:
            try:
                ad = Advert(ad_response)
                yield ad
            except Exception as e:
                print(e.args)


    def _filter_advert(self, url):
        """ Filter the results further more.

            Spareroom filters are not very accurate.
            We can filter even more manually by scraping
            the result pages.

            Args:
                url: The url of the source to scrape.
        """

        # -- TODO --
        # For now just return True until this is implemented
        # request = self._make_request(url)
        return True

    def search(self):
        """ Perform a search

            Yields:
                advert_url: The urls for the pages that match
                            the search criteria.
        """

        search_results = []

        for response in self._pull_pages():
            for ad in self._get_all_ads(response):
                search_results.append(ad)
                print(ad)

        print(len(search_results))
        # Send an email with the results
        #emailer = Emailer('emailer_config.ini')
        #emailer.send_gmail(''.join(search_results))
